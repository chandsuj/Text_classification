# -*- coding: utf-8 -*-
"""Applied ML_V3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IkXaAnhBgNHekDyBHqHakOgYfE9O94tZ
"""

# install spacy
!pip install --upgrade spacy
# !python -m spacy download en_core_web_sm
!python -m spacy download en_core_web_md

!pip install clean-text

"""**1.1 Import the necessary Python modules**"""

# Load python modules

import numpy as np # numpy is a library that allows us to work with vectors and matrices
import matplotlib.pyplot as plt # visualisation library
import pandas as pd # pandas is a library that allows us to work with DataFrames
import seaborn as sns
import spacy
from cleantext import clean
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, accuracy_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.svm import LinearSVC
from sklearn.neighbors import NearestNeighbors
from sklearn.naive_bayes import MultinomialNB
from scipy import stats
from sklearn.metrics import balanced_accuracy_score
from time import time as tt
from sklearn.model_selection import train_test_split








# On some level, dataframes are enhanced matrices where we have assigned names to each
# row and each column.

# Connect google colab to your google drive.
# Note that you need to be logged in to your google account for this step to work

from google.colab import drive
drive.mount('/content/drive')

# Load the data
data_df= pd.read_csv('/content/drive/MyDrive/comp1804_coursework_dataset_23-24.csv')
# print(data_df)
# data_df

"""**EDA**"""

# Summary statistics for numerical features
print(data_df.describe())

# df.info() also tells you what data type a feature is, like the .dtypes call above
print(data_df.info())

print(f"Number of duplicates before dropping : {data_df.duplicated().sum()}")
# dropping the duplicates.
data_df = data_df.drop_duplicates()
print(f"Number of duplicates after dropping : {data_df.duplicated().sum()}")


# Null values
print(" Null value in dataframe without dropping  without dropping the null\n", data_df.isna().sum())

data_df

"""While exploring the given data set, it shows following things:

9347 rows Ã— 8 columns

Number of duplicates in the  data before dropping : 215
 Null value in dataframe without dropping  without dropping the null par_id                   0
paragraph                0
has_entity               0
lexicon_count            0
difficult_words         18
last_editor_gender       0
category                61
text_clarity          9338
dtype: int64

after dropping duplicates total rows and columns are:[9132 rows x 8 columns]

**Observation from the target variable (category) bar plot:**
There is a class imbalance. The categories are not equally distributed. Categories starting with lower case and upper case are also treated as different categories.
"""

# creating the dataframe for the task1 only taking the features we need for the task one
features_list_task1 = ['paragraph', 'has_entity', 'category']
task1_df = data_df[features_list_task1].copy()
task1_df

task1_df.category.value_counts()

# O/p  this shows category class is imbalanced.
# biographies                             2886
# philosophy                              2511
# programming                             1938
# artificial intelligence                 1527
# movies about artificial intelligence     162
# Philosophy                                13
# Biographies                               13
# Programming                               10
# Artificial intelligence                   10
# Movies about artificial intelligence       1
# Convert 'category' values to lowercase
# task1_df['category'] = task1_df['category'].str.lower()

# Check the value counts after converting to lowercase
print(task1_df['category'].value_counts())

# Calculate category counts
category_counts = task1_df['category'].value_counts().sort_values(ascending=False)

# Plot the distribution of categories
plt.figure(figsize=(6, 6))
sns.barplot(x=category_counts.index, y=category_counts.values)
plt.title('Distribution of Categories')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=90, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

# Initially we observed that biographies and Biographies were treated as the different category
# Also the class is imbalanced .
# Distribution of the category are not equal.

# Calculate category counts
has_entity_counts = task1_df['has_entity'].value_counts().sort_values(ascending=False)

# Plot the distribution of categories
plt.figure(figsize=(6, 6))
sns.barplot(x=has_entity_counts.index, y=has_entity_counts.values)
plt.title('Distribution of has_entity input features')
plt.xlabel('Has_entity')
plt.ylabel('Count')
plt.xticks(rotation=90, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()


# This shows there is presence of the missing data .
# Also dataset has a imbalanced distribution of 'has_entity' values.

# Removing missing data from the paragraph, has entity and category.

task1_df.dropna(subset=['paragraph', 'has_entity','category'], inplace=True)

# Remove duplicates
task1_df.drop_duplicates(inplace=True)

# Check for any remaining missing values and other data quality issues
print("Remaining Missing Values:")
print(task1_df.isnull().sum())

# Check for duplicates after removal
print("\nRemaining Duplicates:", task1_df.duplicated().sum())


# task1_df_clean = task1_df.copy()
# print(task1_df_clean)

# o/p:Remaining Missing Values:
# paragraph     0
# has_entity    0
# category      0
# dtype: int64

# Remaining Duplicate: 0


# [9008 rows x 3 columns]


task1_df

"""**A.Text Pre-Processing**
Here we'll go through the entire text cleaning pipeline:

1. Text cleaning.
1. Tokenization.
1. Lemmatization.
1. Remove stop words, punctuation, etc.
1. Scaling up to collections of documents.

**Text Cleaning.**
"""

# creating new dataframe for the preprocessing
task1_df_text_preprocessing = task1_df.copy()

print('How many texts have the newline character "\\n"?')
#I'm using the backslash symbol twice so that python doesn't actually start a new line! The extra backslash is called an "escape character".

print(task1_df_text_preprocessing['paragraph'].str.contains("\n").sum())

#
from cleantext import clean

def clean_text(x):
  """ Define standard cleaning procedure """
  return clean(x,
    fix_unicode=True,               # fix various unicode errors
    lower=True,                     # change all text to lowercase
    no_line_breaks=True,           # this removes occurrences of the newline character "\n"
    no_punct=False,                 # let's NOT remove punctuations for the time being
    no_urls=True,                  # replace all URLs with a special token (below)
    replace_with_url="",          # we decide to replace urls with nothing
    no_emails=True,                # replace all email addresses with a special token
    replace_with_email="",        # we decide to replace emails with nothing
    no_phone_numbers=True,         # replace all phone numbers with a special token
    replace_with_phone_number="",   # we decide to replace phone numbers with nothing
    lang="en"                       # set to 'de' for German special handling
    )


  # not removing punctiation now will deal with it after splitting in to tokens.

# task1_df_text_preprocessing['clean_paragraph']= task1_df_text_preprocessing['paragraph'].apply(clean_text)

task1_df_text_preprocessing[['clean_paragraph', 'clean_category']] = task1_df_text_preprocessing[['paragraph', 'category']].apply(lambda x: x.apply(clean_text))

task1_df_text_preprocessing

"""**In the following, we will use Spacy tokenizer, so that we can use lemmatization and we will tokenize everything beforehand.**

Trained pipelines are behind spaCy's features and capabilities.

The most important feature in the pipeline is the tokenizer, which is used to split a text document into its constituent tokens. Tokens are often, but not always, equivalent to individual words and symbols. Thinking in terms of tokens allows us to consider each text as a different ordered sequence of a limited number of tokens. Indeed, the set of all unique tokens used across all documents in our dataset is called the "vocabulary".

When working with english text there are three main pipeline often used. These have been trained on the same web data, but are based on statistical models of different sizes. Generally, larger models are more accurate but slower. The pipeline names are:

1. `en_core_web_sm`: this is the pipeline with the smallest models. Start here to see if this is powerful enough.
2. `en_core_web_md`: medium models: use them if you need more powerful models.
3. `en_core_web_lg`: largest models: use them if you need the most powerful models (that is, nothing else is working).

These pipelines work with the English language, but Spacy also has pipelines for other languages!

**Tokenization : by using Spacy**
"""

# Create an NLP pipeline
nlp = spacy.load('en_core_web_md')

task1_df_text_tokenization = task1_df_text_preprocessing.copy()
task1_df_text_tokenization

# # process a text through the NLP pipeline to create a Spacy doc
# doc= nlp(task1_df_text_tokenization.iloc[0]['clean_paragraph'])

# # now the "doc" will contain all the tokens that the text was split into
# # the "doc" itself can be thought of as the collection of these tokens
# # We can see them all by iterating through the document
# print('The original document starts with:')
# print(task1_df_text_tokenization.iloc[0]['clean_paragraph'][0:100])
# print('')

# print("Now let's print its first 21 tokens")
# for i,token in enumerate(doc):
#   print(f"Token number {i} is: '{token}'")
#   if i>20:
#     break

"""**Lemmatization**"""

# # create a Spacy doc
# doc= nlp(task1_df_text_tokenization.iloc[0]['clean_paragraph'])

# print('The original document starts with:')
# print(task1_df_text_tokenization.iloc[0]['clean_paragraph'][0:100])
# print('')

# # check the lemmatized tokens
# print("Now let's print the lemmatized version of its first 21 tokens")
# for i,token in enumerate(doc):
#   print(f"The token and its lemma for token number {i} is: '{token}', '{token.lemma_}'")
#   if i>20:
#     break

# # get the list of stopwords
# spacy_stopwords = nlp.Defaults.stop_words
# print(f"There are {len(spacy_stopwords)} stopwords in Spacy. These are:")
# print(sorted(spacy_stopwords))
# print()

# # create a Spacy doc
# doc= nlp(task1_df_text_tokenization.iloc[0]['clean_paragraph'])

# # check the original text
# print('The original document starts with:')
# print(task1_df_text_tokenization.iloc[0]['clean_paragraph'][0:95])
# print('')

# # check which tokens are stopwords
# print("Now let's print the first 21 tokens and check if they're stopwords")
# for i,token in enumerate(doc):
#   print(f"The token number {i} is '{token}', but is it a stopword? " + ("YES" if token.is_stop else "NO"))
#   if i>20:
#     break

"""We can do something similar with punctuation. Using the attribute `is_punct` we can check whether a given token is a punctuation symbol (like .,!) or not."""

# # create a Spacy doc
# doc= nlp(task1_df_text_tokenization.iloc[0]['clean_paragraph'])

# print('The original document starts with:')
# print(task1_df_text_tokenization.iloc[0]['clean_paragraph'][0:95])
# print('')

# # check which tokens are punctuation symbols
# print("Now let's print the first 21 tokens and check if they're punctuation symbols")
# for i,token in enumerate(doc):
#   print(f"The token number {i} is '{token}', but is it a punctuation symbol? " + ("YES" if token.is_punct else "NO"))
#   if i>20:
#     break

"""If we want to process a document so that all punctuation and stop words are removed, we can cycle through the tokens and only keep those that are NOT stopwords and are NOT punctuation. If we want to keep the original tokens, we collect `tokens.text`. If we want to keep its lemmatized version we collect `tokens.lemma_`. Like this:"""

# def process_text(text):
#     doc = nlp(text)
#     processed_tokens_list = [token.lemma_ for token in doc if not (token.is_stop or token.is_punct)]
#     processed_doc = ' '.join(processed_tokens_list)
#     return processed_doc

# task1_df_text_tokenization['lemmatized_paragraph'] = task1_df_text_tokenization['clean_paragraph'].apply(lambda x: process_text(x))

# # print(task1_df_text_tokenization.head())
# task1_df_text_tokenization

# let's create a nice function to tokenize a single text document
import string
EXTRA_PUNCT = string.punctuation

# import Spacy
import spacy
# create the Spacy pipeline
nlp = spacy.load('en_core_web_md')

def preprocess_text_with_spacy(text_):
  """
  This function takes a Spacy doc and returns the list of its lemmas,
  after removing stop words and punctuations
  """
  # process document with Spacy
  # Note that if we were to first run all the documents through Spacy
  # (remember the nlp.pipe()) it would be faster because Spacy processes
  # multiple documents in parallel. However we'd have to create an intermediate
  # variable or column where to store all the Spacy objects, then process them
  # one by one and add the results to our dataframe.
  doc_ = nlp(text_)
  # here we take the lemmas, and now we also want to but only keep those that are NOT stop words, only digits, or punctuation.
  lemmas_ = [token.lemma_ for token in doc_ if not (token.is_stop or token.is_punct or token.is_digit)]
  # remember when I said punctuation is tricky?
  # Spacy misses some characters that we want to remove that can also be considered punctuation (= and +)
  # Here is where we remove them
  return [lemma for lemma in lemmas_ if lemma not in EXTRA_PUNCT]

from time import time as tt
t0 = tt()
task1_df_text_tokenization['tokenized_paragraph'] = task1_df_text_tokenization['clean_paragraph'].apply(preprocess_text_with_spacy)
print(f'Time elapsed is {(tt()-t0):.2f} seconds')

task1_df_text_tokenization

# import CountVectorizer to see ngrams for visualization only
from sklearn.feature_extraction.text import CountVectorizer

# define a function to get the most frequent ngrams
def get_top_ngram(corpus, ngram_range=(1,2), top_n= 50):
    '''
    The input is a collection of text documents, each represented by a list of tokens
    Exactly what we get out of the Spacy preprocessing
    By default it return the top 50 unigrams/bigrams
    '''
    # Let's create our CountVectorizer object
    # We set lowercase= False because we don't need to make tokens lowercase again
    # Since we know we will be using text that has already been tokenized, we need
    # to create our own tokenizer that, basically, does nothing!
    # This is why the tokenizer is the identity function (lambda x: x) whose
    # output is exactly the same as its input (no change needed!)
    # We set max_features to "top_n" because we only want the most common n-grams
    # ngram_range=(1,2) means that we ask CountVectorizer to create unigrams and bi-grams
    vec = CountVectorizer(ngram_range=ngram_range, max_features=top_n, lowercase=False,
                          tokenizer=lambda x: x)
    '''
    NOTE:
    If we were using scikit-learn in-built tokenizer, we would write the CountVectorizer as below.
    No need to specify the tokenizer anymore (we're using the default), but we need to be
    specific about removing stop-words (and also lowercasing).
    vec = CountVectorizer(ngram_range=ngram_range, max_features=top_n, lowercase=True,
                          stop_words = "english")

    '''
    # We now fit the CountVectorizer to our corpus - this function creates the vocabulary
    # and counts how often each n-gram appears
    vec.fit(corpus)
    # get a vector for each document
    bag_of_words = vec.transform(corpus)
    # sum across documents
    sum_words = bag_of_words.sum(axis=0)
    # get the words with their frequencies
    # the attribute .vocabulary stores all the n-grams in the vocabulary, indexed by a number
    words_freq = [(word, sum_words[0, idx])
                  for word, idx in vec.vocabulary_.items()]
    # sort them by frequency
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse= True)
    # separate the n_grams from their frequencies
    ngram_labels = [word[0] for word in words_freq]
    ngram_freqs = [word[1] for word in words_freq]
    # return the results
    return bag_of_words, ngram_freqs, ngram_labels

# let's get the n-gram frequencies
bag_of_words, ngram_freqs, ngram_labels = get_top_ngram(task1_df_text_tokenization['tokenized_paragraph'],
                                                        ngram_range=(1,2), top_n= 50)

# let's plot them (most common ones are at the bottom)
plt.figure(figsize=(7,12))
_ = plt.barh(y=range(50),width=ngram_freqs,tick_label=ngram_labels)

# Most common bigrams for sci.med articles
_, pos_bigram_freqs, pos_bigram_labels= get_top_ngram(task1_df_text_tokenization['tokenized_paragraph'],
                                  ngram_range=(2,2), top_n= 50)

# let's plot them
plt.figure(figsize=(5,9))
_ = plt.barh(y=range(50),width=pos_bigram_freqs,tick_label=pos_bigram_labels)

"""### Distribution of text lengths

An important aspect to consider when dealing with text is getting information about the length of the documents.

Since we have the text already in tokenized form, we will quantify length here as the number of tokens in a document.

You could also use the number of characters, though results can be harder to interpret.
"""

# get the length of the list of tokens
task1_df_text_tokenization['text_length'] = task1_df_text_tokenization['tokenized_paragraph'].apply(len)
# task1_df_text_tokenization

# let's create a nice function to tokenize a single text document
import string
EXTRA_PUNCT = string.punctuation

# import Spacy
import spacy
# create the Spacy pipeline
nlp = spacy.load('en_core_web_md')


# Let's make the easy function first, where we simply return Spacy's default document embedding.
def get_spacy_doc_embedding(text_):
  ''' Given a text document, return the Spacy document embedding. '''
  doc_ = nlp(text_)
  return doc_.vector

# embed training data (it takes a bit more than 2 minutes! A bit of patience is needed)
# (use the cleaned text!)
from time import time as tt
t0 = tt()
# Note a couple of new things in the following application of the apply function:
# 1. The resulting embedding for each row is a list of number.
# 2. Since the plan is to join it back to the original dataframe, we want the result
# to be returned as a dataframe itself to make things easier later on (which is why we wrap the result into pd.Series())

# Join the list of tokens into a single string
task1_df_text_tokenization['tokenized_paragraph_str'] = task1_df_text_tokenization['tokenized_paragraph'].apply(lambda x: ' '.join(x))

task1_df_emb = task1_df_text_tokenization['tokenized_paragraph_str'].apply(lambda x: pd.Series(get_spacy_doc_embedding(x)))

# 3. since they have the same index we can join the dataframe with the word embeddings with the old one
task1_df_new = task1_df_emb.join(task1_df_text_tokenization[['tokenized_paragraph','clean_paragraph','clean_category','has_entity']])
# the next line is just because scikit-learn doesn't like column names that are not strings
task1_df_new.columns = task1_df_new.columns.astype(str)
print(f'Time elapsed is {(tt()-t0):.2f} seconds')
task1_df_new

print(task1_df_new['has_entity'].unique())

# one_hot_encode_data = pd.get_dummies(task1_df_new, columns = ['has_entity'])
# print(one_hot_encode_data)

# one hot encoding of the has_entity
task1_df_new['has_entity_org'] = task1_df_new['has_entity'].apply(lambda x: 1 if 'ORG_YES_' in x else 0)
task1_df_new['has_entity_prod'] = task1_df_new['has_entity'].apply(lambda x: 1 if 'PRODUCT_YES_' in x else 0)
task1_df_new['has_entity_person'] = task1_df_new['has_entity'].apply(lambda x: 1 if 'PERSON_YES_' in x else 0)

# print(task1_df_new)

new_df_category = task1_df_new.copy()


from sklearn.preprocessing import LabelEncoder
lblEncoder_X = LabelEncoder()
lblEncoder_X = lblEncoder_X.fit(new_df_category['clean_category'])

new_df_category['label_category'] = lblEncoder_X.transform(new_df_category['clean_category'])
print(new_df_category['label_category'].value_counts())


# new_df_category





# creating new dataframe only by taking needed features and labels.
task1_model_ready_data = new_df_category.copy()

task1_model_ready_data.drop(columns=['tokenized_paragraph', 'clean_paragraph','has_entity','clean_category'], inplace=True)
task1_model_ready_data

# print(task1_data_ready_to_split.describe())
# plt.boxplot(task1_data_ready_to_split['0'])

# task1_df_new['clean_category'].value_counts()
# task1_df_new_copy['label_category'].value_counts()

task1_model_ready_data['label_category'].value_counts()
# we will check this after splitting the dataset n check imbalance.

# task1_data_ready_to_split DataFrame resuffle
shuffled_df = task1_model_ready_data.sample(frac=1.0, random_state=42)  # Set random_state for reproducibility

# dataframe before suffling
# shuffled_df
# Reset the index if needed
shuffled_df.reset_index(drop=True, inplace=True)

# Display the shuffled DataFrame
shuffled_df

"""# Data Split for task1.

Data splitting divides a dataset into three main subsets: the training set, used to train the model; the validation set, used to track model parameters and avoid overfitting; and the testing set, used for checking the modelâ€™s performance on new data. Each subset serves a unique purpose in the iterative process of developing a machine-learning model
"""

#  splitting the data set
from sklearn.model_selection import train_test_split

# X contains features, y contains labels
X= shuffled_df.drop(columns=['label_category'], inplace=False)
# print(task1_data_ready_to_split)
y = shuffled_df['label_category']  # Specify the column containing the target variable

# print(X,y)

# Splitting data in to train test and validation in the ratio of 60:20:20.
# Split the data into training and test sets
X_train_before, X_test, y_train_before, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Further split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_before, y_train_before, test_size=0.25, random_state=42)

# how many labels do we have in the target and what is their distribution?
print('Label distribution for training data:')
print(y_train.value_counts())

# How about the validation dataset?
print('Label distribution for validation data:')
print(y_val.value_counts())

# How about the test dataset?
print('Label distribution for test data:')
print(y_test.value_counts())

from imblearn.over_sampling import RandomOverSampler

# Check class distribution before applying any balancing technique
print("Before balancing:")
print("Training set class distribution:", np.bincount(y_train))
print("Validation set class distribution:", np.bincount(y_val))
print("Test set class distribution:", np.bincount(y_test))

# Applying RandomOverSampler to balance the training data
oversampler = RandomOverSampler(random_state=0)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)

# Check class distribution after applying balancing technique
print("After balancing:")
print("Training set class distribution:", np.bincount(y_train_resampled))
print("Validation set class distribution:", np.bincount(y_val))
print("Test set class distribution:", np.bincount(y_test))

# Resampling techniques applied to validation or test datasets can inadvertently leak information
# about the distribution of classes in those datasets to the model during training or hyperparameter tuning,
# leading to overfitting or biased performance estimates.

"""**ML PIPELINE**

"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

ct = ColumnTransformer(
    transformers= [
        (
            "scaling", # --> name of the transformation
            StandardScaler(), # --> main function to apply
            [str(i) for i in range(300)], #-->columns to apply it to (we can give more than one column at once!)
        )
    ],
    remainder="passthrough", #--> what to do with the non-transformed columns. passthrough=keep them
    verbose_feature_names_out=False #--> this keeps columns names simple.
)

from sklearn.neural_network import MLPClassifier

# Create the pipeline
clf_ = Pipeline(
        steps =[('encoding', ct),
              ('clf', MLPClassifier()),
      ]
    )

# Set up the hyper-parameters to test
hparameters = {'clf__hidden_layer_sizes': ((20,20,),(30,), (40, 40,)),
              'clf__learning_rate_init': [0.0001, 0.001],
             }

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from time import time as tt


# create the GridSearch function
# original data
clf_search = GridSearchCV(clf_, hparameters, scoring= "accuracy", cv= 5,
                          return_train_score=True)
#with the return_train_score=True setting we also get the performance results on the
# training folds. It makes things slower, but gives us information to check for things
# like over/under-fitting.

# Fit all the possible hyper-parameters combinations using cross-validation.
# It may take some time: be patient
# original data
# let's also time it (it's goint to take time, nothing's wrong!)
t0 = tt()
_ =clf_search.fit(X_train_resampled, y_train_resampled)
print(f'Time taken to train gridsearch: {tt()-t0:.2f} seconds.')

# evaluation
cv_res = pd.DataFrame(clf_search.cv_results_)
interesting_columns = ['mean_test_score','std_test_score','mean_train_score','std_train_score']
interesting_columns = interesting_columns + [t for t in cv_res.columns if 'param_' in t]
cv_res[interesting_columns]

# Get the best estimator for further analysis of the results using the test set
# Does it still perform well?
best_clf = clf_search.best_estimator_
print(best_clf)

# validation
# Compute predictions and evaluation metrics using the best estimator
# original data
y_validation = best_clf.predict(X_val)
print(classification_report(y_val, y_validation, target_names = ['artificial intelligence', 'biographies', 'philosophy','programming','movies about artificial intelligence'] ))

cm= ConfusionMatrixDisplay.from_estimator(best_clf, X_val, y_val)

# testing
# Compute predictions and evaluation metrics using the best estimator
# original data
y_test_predicted = best_clf.predict(X_test)
print(classification_report(y_test, y_test_predicted, target_names = ['artificial intelligence', 'biographies', 'philosophy','programming','movies about artificial intelligence'] ))

cm= ConfusionMatrixDisplay.from_estimator(best_clf, X_test, y_test)

from sklearn.dummy import DummyClassifier
trivial_clf = DummyClassifier(strategy="uniform")
trivial_clf.fit(X_train, y_train)

y_trivial = trivial_clf.predict(X_test)
print(classification_report(y_test, y_trivial, target_names = ['artificial intelligence', 'biographies', 'philosophy','programming','movies about artificial intelligence'] ))

cm= ConfusionMatrixDisplay.from_estimator(trivial_clf,X_test , y_test)

from sklearn.svm import SVC

# Create the pipeline with SVM classifier
svm_clf = Pipeline(
    steps=[
        ('encoding', ct),
        ('clf', SVC()),
    ]
)

# Set up the hyper-parameters to test for SVM
svm_hparameters = {
    'clf__kernel': ['linear', 'rbf'],
    'clf__C': [0.1, 1, 10],
}

# create the GridSearch function for SVM
svm_clf_search = GridSearchCV(svm_clf, svm_hparameters, scoring="accuracy", cv=5,
                              return_train_score=True)

# Fit all the possible hyper-parameters combinations using cross-validation.
t0 = tt()
_ = svm_clf_search.fit(X_train_resampled, y_train_resampled)
print(f'Time taken to train gridsearch: {tt()-t0:.2f} seconds.')

# evaluation
cv_res = pd.DataFrame(svm_clf_search.cv_results_)
interesting_columns = ['mean_test_score','std_test_score','mean_train_score','std_train_score']
interesting_columns = interesting_columns + [t for t in cv_res.columns if 'param_' in t]
cv_res[interesting_columns]

# Get the best estimator for further analysis of the results using the test set
# Does it still perform well?
best_clf = svm_clf_search.best_estimator_
print(best_clf)

# validation
# Compute predictions and evaluation metrics using the best estimator
# original data
y_validation = best_clf.predict(X_val)
print(classification_report(y_val, y_validation, target_names = ['artificial intelligence', 'biographies', 'philosophy','programming','movies about artificial intelligence'] ))

cm= ConfusionMatrixDisplay.from_estimator(best_clf, X_val, y_val)

# # testing
# Compute predictions and evaluation metrics using the best estimator
# original data
y_test_predicted = best_clf.predict(X_test)
print(classification_report(y_test, y_test_predicted, target_names = ['artificial intelligence', 'biographies', 'philosophy','programming','movies about artificial intelligence'] ))

cm= ConfusionMatrixDisplay.from_estimator(best_clf, X_test, y_test)

from sklearn.dummy import DummyClassifier
trivial_clf = DummyClassifier(strategy="uniform")
trivial_clf.fit(X_train, y_train)

y_trivial = trivial_clf.predict(X_test)
print(classification_report(y_test, y_trivial, target_names = ['artificial intelligence', 'biographies', 'philosophy','programming','movies about artificial intelligence'] ))

cm= ConfusionMatrixDisplay.from_estimator(trivial_clf,X_test , y_test)